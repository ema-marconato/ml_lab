{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UODxUxoI-f9G"
   },
   "source": [
    "# Reinforcement Learning Lab\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/0*WC4l7u90TsKs_eXj.png\" width=500px>\n",
    "\n",
    "\n",
    "In this notebook, we explore different strategy for Reinforcement Learning for a simple control task, Cart Pole, first introduced by Sutton, Barto and Anderson [1]. We will mainly use PyTorch and Gymnasium, a popular library containing some basic building blocks for training RL agent.\n",
    "\n",
    "- Gym: https://gymnasium.farama.org/\n",
    "\n",
    "## Quick-start Setup\n",
    "```bash\n",
    "conda create --name ml_labs python=3.10\n",
    "conda activate ml_labs\n",
    "conda install -c conda-forge jupyterlab scikit-learn pandas\n",
    "pip install matplotlib gymnasium tqdm\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "### Structure\n",
    "- SARSA\n",
    "- Q-Learning\n",
    "- Deep Q-Learning\n",
    "\n",
    "[1] A. G. Barto, R. S. Sutton and C. W. Anderson, \"Neuronlike adaptive elements that can solve difficult learning control problems,\" in IEEE Transactions on Systems, Man, and Cybernetics, vol. SMC-13, no. 5, pp. 834-846, Sept.-Oct. 1983, doi: 10.1109/TSMC.1983.6313077.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AFRypAHAkM4"
   },
   "source": [
    "## Cart Pole Environment\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/cart_pole.gif\" height=\"200\" />\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "- **Action Space**: push right or push left the cart;\n",
    "- **Observation Space**: cart position and velocity, pole angle and velocity. All the features are continuous.\n",
    "- **Reward**: +1 for each timestep we keep the pole upwards (in a certain angle). We have a maximum of 500 timestep.\n",
    "\n",
    "An **episode** ends when either: make the pole fall, the cart reaches the edges of the line, or we reach 500 timesteps.\n",
    "\n",
    "You can find the reference implementation [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vv1As-Y6SpyH"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uilm0RdfDXy3"
   },
   "source": [
    "Let us start by first importing the gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxiu1hT8CJt0"
   },
   "outputs": [],
   "source": [
    "# We create an environment\n",
    "env = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6NYItX0CSLp",
    "outputId": "36960633-10bc-4127-ec5f-4f423f7b8673"
   },
   "outputs": [],
   "source": [
    "# Here we show some basic details on an Gym environment\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntmUBCuICqoa"
   },
   "outputs": [],
   "source": [
    "# We reset the state to a new one\n",
    "state, _ = env.reset()\n",
    "\n",
    "# We perform a certain action and we get some info\n",
    "new_state, reward, done, truncated, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZDbgYmYT_dq"
   },
   "source": [
    "SARSA and Q-Learning estimates a action-value function $Q(s,a)$. During the lecture, we usually assume the environment is discrete in order for $Q(s,a)$ to be learnable. Here, Cart Pole has a **continuous** environment instead, thus we need to perform a **discretization** step.\n",
    "\n",
    "There are more sofisticated techniques for discretization (such as Tile Coding [2]) but we will not cover them here.\n",
    "\n",
    "[2] http://www.incompleteideas.net/book/8/node6.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXix9N00qFp0"
   },
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random.seed(2024)\n",
    "np.random.seed(2024)\n",
    "\n",
    "def discretize(env, num_bins=10):\n",
    "  \"\"\"Discretize each feature into a given number of bins\"\"\"\n",
    "  bins = []\n",
    "  for feature_low, feature_high in zip(env.observation_space.low, env.observation_space.high):\n",
    "    bins.append(\n",
    "        np.linspace(max(-10, feature_low), min(10, feature_high), num=num_bins)\n",
    "    )\n",
    "  return bins\n",
    "\n",
    "def convert_state(state, bins):\n",
    "  \"\"\"Given a continuous state, return its binned representation\"\"\"\n",
    "  new_state = []\n",
    "  for feature, feature_bins in zip(state, bins):\n",
    "    new_state.append(\n",
    "        np.digitize(feature, feature_bins)\n",
    "    )\n",
    "  return tuple(new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHkHZFaCUzCN"
   },
   "source": [
    "Let us test our new function before actually using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mzp3xONI1zJ",
    "outputId": "3680675e-4268-4058-c552-ca7ca92a5d2d"
   },
   "outputs": [],
   "source": [
    "bins = ...\n",
    "state, _ = env.reset()\n",
    "convert_state(state, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6cUd0VnVv90"
   },
   "source": [
    "Let us now define a function which, given $Q(s,a)$ will play an episode of the environment. It will be useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pfEbZ3CVAGm"
   },
   "outputs": [],
   "source": [
    "def play(env, Q, bins):\n",
    "  \"\"\"Play an episode using the state-action function Q\"\"\"\n",
    "\n",
    "  ...\n",
    "\n",
    "  while not done and not truncated:\n",
    "\n",
    "    # env.render() Uncomment this line to see an interactive visualization of the agent\n",
    "    # in the environment. It works only if this notebook is run as a simple python\n",
    "    # script on your local environment.\n",
    "\n",
    "    ...\n",
    "\n",
    "  return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFxKhDc3YlNL"
   },
   "source": [
    "We now define a function to solve the control task by using either SARSA or Q-Learning.\n",
    "\n",
    "### SARSA (State-Action-Reward-Sate-Action)\n",
    "<img src=\"https://tcnguyen.github.io/reinforcement_learning/images/SARSA_algo.png\" height=\"300\" />\n",
    "\n",
    "### Q-Learning\n",
    "<img src=\"https://tcnguyen.github.io/reinforcement_learning/images/Q_learning_algo.png\" height=\"300\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeA7u0dCxsFC"
   },
   "outputs": [],
   "source": [
    "def SARSA(Q, bins, alpha=0.25, gamma=0.9, epsilon=0.3, episodes=100):\n",
    "  \"\"\"Implement the SARSA on-policy algorithm\"\"\"\n",
    "\n",
    "  ...\n",
    "\n",
    "  return Q, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bqmjQ_C6V0t"
   },
   "outputs": [],
   "source": [
    "def QLearning(Q, bins, alpha=0.25, gamma=0.9, epsilon=0.3, episodes=100):\n",
    "  \"\"\"Implement the Q-Learning off-policy algorithm\"\"\"\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guWyohIRatMY"
   },
   "source": [
    "Let us try to learn something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niqiW6PyL6kq"
   },
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Pick the high/low environment values\n",
    "bins = ...\n",
    "\n",
    "# Initialize value function\n",
    "Q = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vdJktryQ9vu",
    "outputId": "40b47257-85ec-4542-d755-09aef3aa3c9a"
   },
   "outputs": [],
   "source": [
    "Q_sarsa, eval_sarsa = SARSA(np.copy(Q), bins, episodes=5000)\n",
    "Q_qlearn, eval_qlearning = QLearning(np.copy(Q), bins, episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJtlLvZwavvx"
   },
   "source": [
    "Given the evaluation results, we plot them and compare the two runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "UxED00otPlAd",
    "outputId": "b5a0458b-890a-465a-b571-ffbed3b7683f"
   },
   "outputs": [],
   "source": [
    "eval_sarsa = pd.DataFrame(eval_sarsa, columns=[\"episode\", \"reward\"])\n",
    "eval_qlearning = pd.DataFrame(eval_qlearning, columns=[\"episode\", \"reward\"])\n",
    "\n",
    "sns.lineplot(data=eval_sarsa, x=\"episode\", y=\"reward\", label=\"SARSA\")\n",
    "sns.lineplot(data=eval_qlearning, x=\"episode\", y=\"reward\", label=\"Q_learning\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTHr2oElzl0y"
   },
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "Deep Q-Learning is a technique to learn optimal policies which achieves superhuman performance on many Atari games [3]. It uses a neural network (NN) as a function approximator to learn the state-value utility function $Q(s,a)$ without resorting to discretization. Moreover, they exploit a techinque called _experience replay_ [4] where we store past episode of the agents to train the policy\n",
    "\n",
    "The main components of such architecture are:\n",
    "- A Feed-Forward neural network (3 layers, ReLu activations);\n",
    "- A replay buffer containing the past experiences;\n",
    "\n",
    "[3] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013).\n",
    "\n",
    "[4] Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC\n",
    "Document, 1993."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjjvLd6xw-fw"
   },
   "source": [
    "We want to train a neural network to minimize the following loss function:\n",
    "$$ \\mathcal L(\\theta) = [ (r + \\gamma\\cdot\\max_{a_{t+1} } Q(s_{t+1}, a_{t+1}; \\theta^{target})) - Q(s_t, a_t; \\theta^{pred}  )  ]^2  $$\n",
    "\n",
    "The schematic is as follow:\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:640/format:webp/1*nxn5uxtVBUniW1sTzjNbqQ.png'>\n",
    "\n",
    "We first predict the Q-value associated to the current state $s_t$ and the chosen action $a_t$, then by oberving the next state $s_{t+1}$ of the environment the Q-value is computed by chosing the optimal action $a_{t+1}$.\n",
    "\n",
    "In principle, $\\theta^{pred}$ and $\\theta^{target}$ can be two different neural networks, to avoid divergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbObPQWo0FEk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC9qh7Fwcy27"
   },
   "source": [
    "We define a replay buffer, which will contain all the agent states and rewards it experiences over training. This aids training effectively the Deep Q-Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipr-acMB3e9F"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer which will store all the experiences made by the agent\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.storage = []\n",
    "\n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "\n",
    "        self.storage.append([\n",
    "            state, action, next_state, reward, done\n",
    "        ])\n",
    "\n",
    "        if len(self.storage) > self.capacity:\n",
    "            self.storage.pop()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "      batch = np.random.randint(0, len(self), batch_size).tolist() if len(self) >= batch_size else list(range(len(self)))\n",
    "\n",
    "      x, y, u, r, d = [], [], [], [], []\n",
    "\n",
    "      for i in batch:\n",
    "          X, Y, U, R, D = self.storage[i]\n",
    "          x.append(np.array(X, copy=False))\n",
    "          y.append(np.array(Y, copy=False))\n",
    "          u.append(np.array(U, copy=False))\n",
    "          r.append(np.array(R, copy=False))\n",
    "          d.append(np.array(D, copy=False))\n",
    "\n",
    "      return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1,1), np.array(d).reshape(-1,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgUTqCTfc7L6"
   },
   "source": [
    "We then define our Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqgiYqI90EAA"
   },
   "outputs": [],
   "source": [
    "# Define Q-network\n",
    "class QNetwork(nn.Module):\n",
    "  \"\"\"Q-Network component we will learn\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(QNetwork, self).__init__()\n",
    "    self.fc = nn.Linear(input_size, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.fc(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TroB_UUIdC2A"
   },
   "source": [
    "At last, we can combine all this pieces to build the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98cjXl-20gZL"
   },
   "outputs": [],
   "source": [
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, hidden_size, output_size, epsilon=0.3, lr=1e-3, gamma=0.9):\n",
    "      \"\"\"Initialize all the components of the agent\"\"\"\n",
    "\n",
    "      self.q_network = QNetwork(input_size, hidden_size, output_size)\n",
    "      self.loss = nn.MSELoss()\n",
    "      self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "      self.gamma = gamma\n",
    "      self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self, state, greedy=False):\n",
    "      \"\"\" Implement an epsilon-greedy action selection policy\"\"\"\n",
    "      if np.random.random() <= self.epsilon and greedy:\n",
    "            return torch.tensor([np.random.randint(0, len(self.q_network(state)))])\n",
    "      else:\n",
    "          with torch.no_grad():\n",
    "            return  torch.tensor([torch.argmax(self.q_network(state))])\n",
    "\n",
    "    def update_q_network(self, batch):\n",
    "      \"\"\" Perform an optimization step given an experience batch\"\"\"\n",
    "\n",
    "      state_batch, action_batch, next_state_batch, reward_batch, done_batch = batch\n",
    "\n",
    "      state_batch = torch.FloatTensor(state_batch)\n",
    "      next_state_batch = torch.FloatTensor(next_state_batch)\n",
    "      action_batch = torch.LongTensor(action_batch)\n",
    "      reward_batch = torch.FloatTensor(reward_batch)\n",
    "      done_batch = torch.FloatTensor(1-done_batch)\n",
    "\n",
    "      ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1h-qdKedHzG"
   },
   "source": [
    "Let us write the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdFGON_BR82G"
   },
   "outputs": [],
   "source": [
    "def deepQ(agent, env, buffer, num_episodes=500, batch_size=128):\n",
    "  \"\"\"Method which train a DQN agent.\"\"\"\n",
    "\n",
    "  ...\n",
    "\n",
    "  return eval_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTIT22aGdLK9"
   },
   "source": [
    "Great! Now it is time to test our creation on the Cart Pole control problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srcxf0BjklF8"
   },
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = DQNAgent(input_size=env.observation_space.shape[0], hidden_size=64, epsilon=0.3, output_size=env.action_space.n)\n",
    "\n",
    "# Replay buffer\n",
    "buffer = ReplayBuffer(15000)\n",
    "\n",
    "# Training\n",
    "num_episodes = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJwcynfuR503",
    "outputId": "ea9d2cad-5af7-4d1f-a2ca-7346a96bada8"
   },
   "outputs": [],
   "source": [
    "eval_deepQ = deepQ(agent, env, buffer, num_episodes, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bodQL0gYdSbf"
   },
   "source": [
    "Then, we just print again the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "MkkHGiViS0vD",
    "outputId": "27b275e1-261f-428b-d497-2edf00b422a0"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eval_deepQ = pd.DataFrame(eval_deepQ, columns=[\"episode\", \"reward\"])\n",
    "\n",
    "sns.lineplot(data=eval_sarsa[eval_sarsa.episode <= num_episodes], x=\"episode\", y=\"reward\", label=\"SARSA\")\n",
    "sns.lineplot(data=eval_qlearning[eval_sarsa.episode <= num_episodes], x=\"episode\", y=\"reward\", label=\"Q_learning\")\n",
    "sns.lineplot(data=eval_deepQ, x=\"episode\", y=\"reward\", label=\"Deep Q-learning\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mMK3L1e1fvz"
   },
   "source": [
    "## Additional Exercises\n",
    "\n",
    "- Pick a new control environment from Gymnasium (e.g., Mountain Car) and try to learn an optimal state-action function.\n",
    "- Implement Tile Coding to solve Cart Pole with SARSA/Q-Learning and compare it with Deep Q-Learning.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
