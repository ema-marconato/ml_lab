{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "<!--<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:1440/1*YUl_BcqFPgX49sSb5yrk3A.jpeg\" style=\"width: 1000px;\">-->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We are going to see some techniques for unsupervised learning. In this setting, labels for classification or values for regression are not available and possible values are left to be discovered by the model. We will make use of modules and classes in the **sklearn** library, whereas more advanced methods based on Neural Networks can be found here:\n",
    "- https://www.analyticsvidhya.com/blog/2018/05/essentials-of-deep-learning-trudging-into-unsupervised-deep-learning/\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Python (preferably version > 3.7): https://www.python.org/downloads/\n",
    "2. Numpy, Scipy and Matplotlib: https://www.scipy.org/install.html\n",
    "3. Scikit-learn: http://scikit-learn.org/stable/install.html\n",
    "4. Pandas: https://pandas.pydata.org/docs/getting_started/index.html\n",
    "\n",
    "## Quick-start Setup\n",
    "```bash\n",
    "conda create --name ml_labs python=3.10\n",
    "conda activate ml_labs\n",
    "conda install -c conda-forge jupyterlab scikit-learn pandas\n",
    "pip install matplotlib\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "- https://docs.scipy.org/doc/numpy/\n",
    "- https://docs.scipy.org/doc/scipy/reference/\n",
    "- https://matplotlib.org/users/index.html\n",
    "- http://scikit-learn.org/stable/documentation.html\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "- ### Class discovery:\n",
    "    - #### K-means clustering\n",
    "    - #### Gaussian Mixture model\n",
    "- ### Dimensionality reduction:\n",
    "    - #### Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a couple of functions which will be useful to plot the decision function of a trained ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings within the notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Classification\n",
    "\n",
    "<img src=\"img/kmean.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Given some data ($X \\in \\mathbf{R}^d$) and $K \\in \\mathbb{R}^+$ (number of clusters), we want to split $X$ in $K$ partitions: $S_1, S_2, \\dots, S_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "centers = [[1, 1], [-1, -1], [1, -1]] # Centroids of the clusters\n",
    "n_clusters = len(centers)\n",
    "X, labels_true = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_blobs(X, labels_true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each partition $S_i \\subset X$ has a center point (centroid $c_i$). We would like to solve the following optimization problem problem:\n",
    "$$ \n",
    "    arg min_S \\sum_{i=1}^K \\sum_{x \\in S_i} dist(x, c_i)\n",
    "$$\n",
    "where $c_i$ is the mean of points in $S_i$. \n",
    "Suppose to use the norm 2 distance $dist(x,c_i) = || x - c_i||^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"kmean_steps/1.png\" style=\"width: 300px;\"/> \\\n",
    "\n",
    "\n",
    "<img src=\"kmean_steps/2.png\" style=\"width: 300px;\"/> \\\n",
    "\n",
    "\n",
    "<img src=\"kmean_steps/3.png\" style=\"width: 300px;\"/> \\\n",
    "\n",
    "\n",
    "<img src=\"kmean_steps/4.png\" style=\"width: 300px;\"/> \\\n",
    "\n",
    "\n",
    "<img src=\"kmean_steps/5.png\" style=\"width: 300px;\"/> \\\n",
    "\n",
    "\n",
    "<img src=\"kmean_steps/6.png\" style=\"width: 300px;\"/> \\\n",
    "\n",
    "\n",
    "<img src=\"kmean_steps/7.png\" style=\"width: 300px;\"/> \\\n",
    "\n",
    "\n",
    "<img src=\"kmean_steps/8.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Call KMeans by choosing also how to initialize, number of cluster, and number of restarts\n",
    "k_means = ...\n",
    "\n",
    "# start counting time\n",
    "t0 = time.time()\n",
    "\n",
    "# We fit K-means using the data\n",
    "k_means.fit(X)\n",
    "\n",
    "# end of training\n",
    "t_batch = time.time() - t0\n",
    "\n",
    "print('Required training time:', '{:.3f}'.format(t_batch), 'sec.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "batch_size = 45\n",
    "\n",
    "# This variant implements KMeans by selecting mini-batches \n",
    "mbk = ...\n",
    "\n",
    "# start counting time\n",
    "t0 = time.time()\n",
    "\n",
    "mbk.fit(X)\n",
    "\n",
    "# end of training\n",
    "t_mini_batch = time.time() - t0\n",
    "\n",
    "print('Required training time:', '{:.3f}'.format(t_mini_batch), 'sec.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "\n",
    "# evaluate centers for two variants\n",
    "k_means_cluster_centers = k_means.cluster_centers_\n",
    "mbk_means_cluster_centers = mbk.cluster_centers_\n",
    "\n",
    "#Â make sure to reorder cluster labels\n",
    "order = pairwise_distances_argmin(k_means.cluster_centers_, mbk.cluster_centers_)\n",
    "mbk_means_cluster_centers = mbk_means_cluster_centers[order]\n",
    "\n",
    "# assign unsupervised labels based on minimum distance from cluster centers\n",
    "k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)\n",
    "mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_K_means(X, 3, \n",
    "             k_means, k_means_labels, k_means_cluster_centers,\n",
    "             mbk, mbk_means_labels, mbk_means_cluster_centers,\n",
    "             t_batch, t_mini_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_kmeans_decision_boundaries\n",
    "\n",
    "# Plot decision boundaries\n",
    "plot_kmeans_decision_boundaries(X, k_means)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many clusters?\n",
    "\n",
    "Suppose we do not know how many clusters should be used in the problem. How can we choose a suitable number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[1, 1], [-1, -1], [1, -1], [-1,1]]\n",
    "n_clusters = len(centers)\n",
    "\n",
    "X, labels_true = ...\n",
    "\n",
    "plot_blobs(X, np.zeros(X.shape[0], dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "wcss, silhuoettes = [], [] \n",
    "models, labels, centers, = [], [], []\n",
    "for i in range(1, 11): \n",
    "    kmeans = ...\n",
    "    kmeans.fit(X) \n",
    "    models.append(kmeans)\n",
    "    labels.append(kmeans.predict(X))\n",
    "    centers.append(kmeans.cluster_centers_)\n",
    "    \n",
    "    wcss.append(kmeans.inertia_)\n",
    "    if i > 1:\n",
    "        silhuoettes.append(silhouette_score(X, labels[-1]) )\n",
    "    else:\n",
    "        silhuoettes.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_clusters(X, labels, centers)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one should we choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.add_subplot(131)\n",
    "\n",
    "plt.plot(range(1, 11), wcss, marker='*')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia') \n",
    "\n",
    "plt.xticks(range(1,11), list(range(1,11)))\n",
    "plt.title('Elbow method')\n",
    "\n",
    "fig.add_subplot(132)\n",
    "plt.plot(range(1, 11), wcss, marker='*')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia') \n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xticks(range(1,11), list(range(1,11)))\n",
    "plt.title('Elbow method (log/log scale)')\n",
    "\n",
    "fig.add_subplot(133)\n",
    "plt.plot(range(1, 11), silhuoettes, marker='*', color='orange')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score') \n",
    "plt.title('Silhouette method')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means work great but has some limitations. Consider the case where we have not-circular shape for the clusters. K-means will struggle to find that there actually three eliptic clusters:\n",
    "\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/10/kmeans-fail-1.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of using a distance-based model, we will now use a distribution-based model. And that is where Gaussian Mixture Models come into play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data with clusters having increased variance along the first component\n",
    "X, y = ...\n",
    "\n",
    "plot_blobs(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gaussian Mixture Model with a specified number of components (clusters)\n",
    "n_components = 4\n",
    "gmm = ...\n",
    "kmeans = ...\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(X)\n",
    "gmm.fit(X)\n",
    "\n",
    "# Predict the cluster labels for each data point\n",
    "k_labels = k_means.predict(X)\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "figure = plt.figure(figsize=(10,5))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "plot_kmeans_decision_boundaries(X, kmeans)\n",
    "plt.title('K-Means clustering')\n",
    "\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=0.5, cmap='viridis')\n",
    "\n",
    "# Plot ellipses for each Gaussian component\n",
    "for i in range(n_components):\n",
    "    plot_cov_ellipse(gmm.covariances_[i], gmm.means_[i], ax=plt.gca(), color='red', alpha=0.2)\n",
    "\n",
    "plt.title('Gaussian Mixture Model Clustering with Ellipses')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving classification problems with GMMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux\n",
    "# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from utils.plot_utils import estimate_GMMS\n",
    "\n",
    "# Load the iris dataset.\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Break up the dataset into non-overlapping training (75%) and testing\n",
    "# (25%) sets.\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Get the total classes\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try GMMs using different types of covariances.\n",
    "estimators = {\n",
    "    cov_type: GaussianMixture(\n",
    "        n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0\n",
    "    )\n",
    "    for cov_type in [\"spherical\", \"diag\", \"tied\", \"full\"]\n",
    "}\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "for index, (name, estimator) in enumerate(estimators.items()):\n",
    "    # Since we have class labels for the training data, we can\n",
    "    # initialize the GMM parameters in a supervised manner.\n",
    "    estimator.means_init = np.array(\n",
    "        [X_train[y_train == i].mean(axis=0) for i in range(n_classes)]\n",
    "    )\n",
    "\n",
    "    # Train the other parameters using the EM algorithm.\n",
    "    estimator.fit(X_train)\n",
    "\n",
    "estimate_GMMS(estimators, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Dataset: Wine Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines [1].\n",
    "\n",
    "[1] https://archive.ics.uci.edu/ml/datasets/wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_wine = pd.read_csv(\"../sklearn/data/wine.csv\")\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into target and features\n",
    "y = df_wine[\"type\"]\n",
    "X = df_wine.drop(columns=[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = ...\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PCA object and transform the data\n",
    "pca = ...\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot the explained variace ratio for each component\n",
    "percent_variance = pca.explained_variance_ratio_\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.add_subplot(121)\n",
    "plt.bar(np.arange(len(percent_variance)), height=percent_variance)\n",
    "plt.ylabel(\"Variance ($\\sigma^2$)\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "\n",
    "fig.add_subplot(122)\n",
    "\n",
    "sum_variance = [np.sum(percent_variance[:i+1]) for i in range(len(percent_variance))]\n",
    "\n",
    "plt.plot(np.arange(len(percent_variance)), sum_variance)\n",
    "plt.ylabel(\"Cumulative Variance\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of inflexion (where the line starts to bend) should indicate how many components have to be retained. In this case, the magic number is 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (2-dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PCA object (2 components) and transform data\n",
    "pca = ...\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data by using the utils.lib.plot_pca_clusters function\n",
    "from utils.plot_utils import plot_pca_clusters\n",
    "plot_pca_clusters(X_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (3-dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PCA object (3 components) and transform data\n",
    "pca = ...\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data by using the utils.lib.plot_pca_clusters function\n",
    "plot_pca_clusters(X_pca, y, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
